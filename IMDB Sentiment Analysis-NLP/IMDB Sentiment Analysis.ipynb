{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Rohit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Rohit/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import html\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Training/Test Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "   row_Number                                               text  polarity\n",
      "0        2148  first think another Disney movie, might good, ...         1\n",
      "1       23577  Put aside Dr. House repeat missed, Desperate H...         0\n",
      "2        1319  big fan Stephen King's work, film made even gr...         1\n",
      "3       13358  watched horrid thing TV. Needless say one movi...         0\n",
      "4        9495  truly enjoyed film. acting terrific plot. Jeff...         1\n",
      "\n",
      "(25000, 2)\n",
      "   row_number                                               text\n",
      "0           0  Oh gosh!! I love movie sooooooooooooooooooooo ...\n",
      "1           1  I saw Borderline several years ago AMC. I've l...\n",
      "2           2  Let say GRANNY extremely well made horror viol...\n",
      "3           3  I like Full Moon Pictures I ordered movie USA,...\n",
      "4           4  Worst horror film ever funniest film ever roll...\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/Rohit/Desktop/imdb\"\n",
    "train_file_name = \"train.csv\"\n",
    "test_file_name  = \"test.csv\"\n",
    " \n",
    "train_data = pd.read_csv(file_path+'/'+train_file_name, encoding='latin-1')\n",
    "print(train_data.shape)\n",
    "print(train_data.head())\n",
    "print(\"\")\n",
    "\n",
    "test_data = pd.read_csv(file_path+'/'+test_file_name, encoding='latin-1')\n",
    "print(test_data.shape)\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the text data and clean the data as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_Number</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>numerics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2148</td>\n",
       "      <td>first think another Disney movie, might good, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>314</td>\n",
       "      <td>5.057692</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23577</td>\n",
       "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>565</td>\n",
       "      <td>5.581395</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1319</td>\n",
       "      <td>big fan Stephen King's work, film made even gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>193</td>\n",
       "      <td>1268</td>\n",
       "      <td>5.575130</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13358</td>\n",
       "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>414</td>\n",
       "      <td>5.587302</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495</td>\n",
       "      <td>truly enjoyed film. acting terrific plot. Jeff...</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>477</td>\n",
       "      <td>6.353846</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_Number                                               text  polarity  \\\n",
       "0        2148  first think another Disney movie, might good, ...         1   \n",
       "1       23577  Put aside Dr. House repeat missed, Desperate H...         0   \n",
       "2        1319  big fan Stephen King's work, film made even gr...         1   \n",
       "3       13358  watched horrid thing TV. Needless say one movi...         0   \n",
       "4        9495  truly enjoyed film. acting terrific plot. Jeff...         1   \n",
       "\n",
       "   word_count  char_count  avg_word  stopwords  numerics  \n",
       "0          52         314  5.057692          1         2  \n",
       "1          86         565  5.581395          2         4  \n",
       "2         193        1268  5.575130          3         1  \n",
       "3          63         414  5.587302          1         0  \n",
       "4          65         477  6.353846          2         0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    " \n",
    "train_data['word_count'] = train_data['text'].apply(lambda x: len(str(x).split(\" \")))   ## Word count\n",
    "train_data['char_count'] = train_data['text'].str.len()                                ## characters count, includes spaces\n",
    "train_data['avg_word']   = train_data['text'].apply(lambda x: avg_word(x)) ## Average Words\n",
    "\n",
    "#Count the number of Stop Words\n",
    "train_data['stopwords']  = train_data['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "\n",
    "#numeric data\n",
    "train_data['numerics'] = train_data['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Pre-processing of the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a Functio to clean the data, remove junk characters, and stem data\n",
    "def cleanse_data(x, freq_com, freq_rare):\n",
    "    re1 = re.compile(r'  +')\n",
    "    # Stemming Data\n",
    "    '''Removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach. For this purpose,\n",
    "       we will use SnowballStemmer from the NLTK library.\n",
    "    '''\n",
    "\n",
    "    # stemmer = SnowballStemmer('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    x = ' '.join([lemmatizer.lemmatize(word) for word in str(x).split(' ')])\n",
    "    \n",
    "    # Remove the junk characters\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    x = re1.sub(' ', html.unescape(x))\n",
    "\n",
    "    #Lower Case the Word\n",
    "    ''' Transform our reviews into lower case. This avoids having multiple copies of the same words.\n",
    "        For example, while calculating the word count, ‘Analytics’ and ‘analytics’ will be taken as different words.\n",
    "    '''\n",
    "    \n",
    "    x = \" \".join(x.lower() for x in x.split())   \n",
    "\n",
    "    # Removing Punctuation\n",
    "    ''' Remove punctuation, as it doesn’t add any extra information while treating text data. Therefore\n",
    "        removing all instances of it will help us reduce the size of the training data.\n",
    "    '''\n",
    "    x = x.replace('[^\\w\\s]','')\n",
    "\n",
    "    # Removal of Stop Words\n",
    "    ''' Stop words (or commonly occurring words) should be removed from the text data. For this purpose,\n",
    "        we can either create a list of stopwords ourselves or we can use predefined libraries.\n",
    "    '''\n",
    "    x = \" \".join(x for x in x.split() if x not in stop)\n",
    "\n",
    "    #Common word removal\n",
    "    ''' Remove commonly occurring words from our text data First, let’s check the 10 most frequently occurring\n",
    "        words in our text data then take call to remove or retain.\n",
    "    '''\n",
    "    x = \" \".join(x for x in x.split() if x not in freq_com)\n",
    "\n",
    "    #Rare words removal\n",
    "    ''' Remove rarely occurring words from the text. Because they’re so rare, the association between them and\n",
    "        other words is dominated by noise.\n",
    "    '''\n",
    "    x = \" \".join(x for x in x.split() if x not in freq_rare)   \n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_Number</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>numerics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2148</td>\n",
       "      <td>first think another disney movie, might good, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>314</td>\n",
       "      <td>5.057692</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23577</td>\n",
       "      <td>put aside dr. house repeat missed, desperate h...</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>565</td>\n",
       "      <td>5.581395</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1319</td>\n",
       "      <td>big fan stephen king's work, made greater fan ...</td>\n",
       "      <td>1</td>\n",
       "      <td>193</td>\n",
       "      <td>1268</td>\n",
       "      <td>5.575130</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13358</td>\n",
       "      <td>watched horrid thing tv. needless say watch se...</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>414</td>\n",
       "      <td>5.587302</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495</td>\n",
       "      <td>truly enjoyed film. acting terrific plot. jeff...</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>477</td>\n",
       "      <td>6.353846</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_Number                                               text  polarity  \\\n",
       "0        2148  first think another disney movie, might good, ...         1   \n",
       "1       23577  put aside dr. house repeat missed, desperate h...         0   \n",
       "2        1319  big fan stephen king's work, made greater fan ...         1   \n",
       "3       13358  watched horrid thing tv. needless say watch se...         0   \n",
       "4        9495  truly enjoyed film. acting terrific plot. jeff...         1   \n",
       "\n",
       "   word_count  char_count  avg_word  stopwords  numerics  \n",
       "0          52         314  5.057692          1         2  \n",
       "1          86         565  5.581395          2         4  \n",
       "2         193        1268  5.575130          3         1  \n",
       "3          63         414  5.587302          1         0  \n",
       "4          65         477  6.353846          2         0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Common word removal\n",
    "''' Remove commonly occurring words from our text data First, let’s check the 10 most frequently occurring\n",
    "    words in our text data then take call to remove or retain.\n",
    "'''\n",
    "freq_com = pd.Series(' '.join(train_data['text']).split()).value_counts()[:10]\n",
    "freq_com = list(freq_com.index)\n",
    "\n",
    "#Rare words removal\n",
    "''' Remove rarely occurring words from the text. Because they’re so rare, the association between them and\n",
    "    other words is dominated by noise.\n",
    "'''\n",
    "freq_rare = pd.Series(' '.join(train_data['text']).split()).value_counts()[-10:]\n",
    "freq_rare = list(freq_rare.index)\n",
    "    \n",
    "#Clean the data remove junk characters, stem data\n",
    "train_data['text'] = train_data['text'].apply(lambda x: cleanse_data(x, freq_com, freq_rare))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Data Modelling and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data['text'], train_data['polarity'], test_size=0.40, random_state=42 )\n",
    "log_loss_scorer = make_scorer(log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, SelectKBest \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.15 s, sys: 263 ms, total: 6.41 s\n",
      "Wall time: 6.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set a reasonable number of features before adding interactions\n",
    "chi_k = 300\n",
    "\n",
    "# create the pipeline object\n",
    "pipeline = Pipeline([('vect', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                non_negative=True,\n",
    "                                                norm=None,\n",
    "                                                binary=False,\n",
    "                                                ngram_range=(1, 2)\n",
    "                                               )),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "pipeline.fit(train_data['text'], train_data['polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9857"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_number</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>labels_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>oh gosh!! love sooooooooooooooooooooo much!!!!...</td>\n",
       "      <td>0.703538</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>saw borderline several year ago amc. i've look...</td>\n",
       "      <td>0.620915</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>let say granny extremely well made horror viol...</td>\n",
       "      <td>0.542915</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>full moon picture ordered usa, germany can't g...</td>\n",
       "      <td>0.353424</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>worst horror ever funniest ever rolled got see...</td>\n",
       "      <td>0.395889</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>first saw teen last year junior high. riveted ...</td>\n",
       "      <td>0.677679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>old jess franco! always-reliable choice direct...</td>\n",
       "      <td>0.352146</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>dogtown z-boyssummary: dogtown z-boys document...</td>\n",
       "      <td>0.682829</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>rigoletto verdi's masterpiece, full drama, emo...</td>\n",
       "      <td>0.440873</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>high expectation dawn. know keep buying slashe...</td>\n",
       "      <td>0.313520</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>delightful wonderful film, entered pantheon gr...</td>\n",
       "      <td>0.744334</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>think samuel goldwyn trying accomplish two thi...</td>\n",
       "      <td>0.704626</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>joseph conrad's timeless novel, heart darkness...</td>\n",
       "      <td>0.666490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>sadly, potential willing cast, everything poor...</td>\n",
       "      <td>0.185715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>opening shot consequence love perfectly set in...</td>\n",
       "      <td>0.647631</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>watched expecting see usual british stiff uppe...</td>\n",
       "      <td>0.566547</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>\"you got beans? bean good. eat go.\" help laugh...</td>\n",
       "      <td>0.329494</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>see intending do. unfortunately never quite co...</td>\n",
       "      <td>0.447633</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>dessert was, must say, worst ever seen. acting...</td>\n",
       "      <td>0.137696</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>czech cinematography traveling dark times... t...</td>\n",
       "      <td>0.397318</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>admit now. lamest ever made. but, mr sjogrens ...</td>\n",
       "      <td>0.539191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>bronson ireland, last together, make likable p...</td>\n",
       "      <td>0.506138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>first saw 1956 though saw recently changed min...</td>\n",
       "      <td>0.677548</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>think it, nearly unbelievable could made death...</td>\n",
       "      <td>0.617181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>great fun evening sofa. expect academy award s...</td>\n",
       "      <td>0.708092</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>thought movie. someone said 'sick' watch it. t...</td>\n",
       "      <td>0.402428</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>ator series shining example b-movies be. fail ...</td>\n",
       "      <td>0.484196</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>great little ground-breaking (in 1955) importa...</td>\n",
       "      <td>0.700368</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>clearly bank marketed exotica audience unfamil...</td>\n",
       "      <td>0.477282</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>great story. although jimmy stewart cornball p...</td>\n",
       "      <td>0.604091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    row_number                                               text    labels  \\\n",
       "0            0  oh gosh!! love sooooooooooooooooooooo much!!!!...  0.703538   \n",
       "1            1  saw borderline several year ago amc. i've look...  0.620915   \n",
       "2            2  let say granny extremely well made horror viol...  0.542915   \n",
       "3            3  full moon picture ordered usa, germany can't g...  0.353424   \n",
       "4            4  worst horror ever funniest ever rolled got see...  0.395889   \n",
       "5            5  first saw teen last year junior high. riveted ...  0.677679   \n",
       "6            6  old jess franco! always-reliable choice direct...  0.352146   \n",
       "7            7  dogtown z-boyssummary: dogtown z-boys document...  0.682829   \n",
       "8            8  rigoletto verdi's masterpiece, full drama, emo...  0.440873   \n",
       "9            9  high expectation dawn. know keep buying slashe...  0.313520   \n",
       "10          10  delightful wonderful film, entered pantheon gr...  0.744334   \n",
       "11          11  think samuel goldwyn trying accomplish two thi...  0.704626   \n",
       "12          12  joseph conrad's timeless novel, heart darkness...  0.666490   \n",
       "13          13  sadly, potential willing cast, everything poor...  0.185715   \n",
       "14          14  opening shot consequence love perfectly set in...  0.647631   \n",
       "15          15  watched expecting see usual british stiff uppe...  0.566547   \n",
       "16          16  \"you got beans? bean good. eat go.\" help laugh...  0.329494   \n",
       "17          17  see intending do. unfortunately never quite co...  0.447633   \n",
       "18          18  dessert was, must say, worst ever seen. acting...  0.137696   \n",
       "19          19  czech cinematography traveling dark times... t...  0.397318   \n",
       "20          20  admit now. lamest ever made. but, mr sjogrens ...  0.539191   \n",
       "21          21  bronson ireland, last together, make likable p...  0.506138   \n",
       "22          22  first saw 1956 though saw recently changed min...  0.677548   \n",
       "23          23  think it, nearly unbelievable could made death...  0.617181   \n",
       "24          24  great fun evening sofa. expect academy award s...  0.708092   \n",
       "25          25  thought movie. someone said 'sick' watch it. t...  0.402428   \n",
       "26          26  ator series shining example b-movies be. fail ...  0.484196   \n",
       "27          27  great little ground-breaking (in 1955) importa...  0.700368   \n",
       "28          28  clearly bank marketed exotica audience unfamil...  0.477282   \n",
       "29          29  great story. although jimmy stewart cornball p...  0.604091   \n",
       "\n",
       "    labels_predict  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                0  \n",
       "4                0  \n",
       "5                1  \n",
       "6                0  \n",
       "7                1  \n",
       "8                0  \n",
       "9                0  \n",
       "10               1  \n",
       "11               1  \n",
       "12               1  \n",
       "13               0  \n",
       "14               1  \n",
       "15               1  \n",
       "16               0  \n",
       "17               0  \n",
       "18               0  \n",
       "19               0  \n",
       "20               1  \n",
       "21               1  \n",
       "22               1  \n",
       "23               1  \n",
       "24               1  \n",
       "25               0  \n",
       "26               0  \n",
       "27               1  \n",
       "28               0  \n",
       "29               1  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['text'] = test_data['text'].apply(lambda x: cleanse_data(x, freq_com, freq_rare))\n",
    "test_data['labels'] = pipeline.predict_proba(test_data['text'])[::,1]\n",
    "test_data['labels_predict'] = pipeline.predict(test_data['text'])\n",
    "test_data.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the Output CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['row_number', 'labels', 'labels_predict']].to_csv(file_path+\"/sample_sub.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
